apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llama-3-8b-pvc-1
  namespace: vllm-llama3-8b-poc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 100Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: setup-llama3-8b-binary-1
  namespace: vllm-llama3-8b-poc
spec:
  volumes:
    - name: model-volume
      persistentVolumeClaim:
        claimName: llama-3-8b-pvc-1
  restartPolicy: Never
  initContainers:
    - name: fix-volume-permissions
      image: quay.io/quay/busybox:latest
      imagePullPolicy: IfNotPresent
      securityContext:
        allowPrivilegeEscalation: true
      resources:
        requests:
          memory: "64Mi"
          cpu: "250m"
        limits:
          memory: "128Mi"
          cpu: "500m"
      command: ["sh"]
      args: ["-c", "chown -R 1001:1001 /mnt/models"]
      volumeMounts:
        - mountPath: "/mnt/models/"
          name: model-volume
  containers:
    - name: download-model
      image: registry.access.redhat.com/ubi9/python-311:latest
      imagePullPolicy: IfNotPresent
      securityContext:
        allowPrivilegeEscalation: true
      resources:
        requests:
          memory: "1Gi"
          cpu: "1"
        limits:
          memory: "1Gi"
          cpu: "1"
      command: ["sh"]
      args:
        [
          "-c",
          "pip install --upgrade pip && pip install --upgrade huggingface_hub && python3 -c 'from huggingface_hub import snapshot_download\nsnapshot_download(\n repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\nlocal_dir=\"/mnt/models/hf/8b_instruction_tuned\",local_dir_use_symlinks=False,use_auth_token=\"${HF_TOKEN}\")'",
        ]
      volumeMounts:
        - mountPath: "/mnt/models/"
          name: model-volume
      env:
        - name: TRANSFORMERS_CACHE
          value: /tmp
