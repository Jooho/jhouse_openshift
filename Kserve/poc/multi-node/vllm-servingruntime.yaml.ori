apiVersion: serving.kserve.io/v1alpha1
kind: ClusterServingRuntime
metadata:
  name: kserve-vllm
spec:
  annotations:
    prometheus.kserve.io/port: "3000"
    prometheus.kserve.io/path: "/metrics"
  supportedModelFormats:
    - name: huggingface
      version: "1"
      autoSelect: true
      priority: 1
  protocolVersions:
    - v2
    - v1
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  - NVIDIA-A10G
  containers:
    - name: kserve-container
      image: kserve/vllm:latest
      command: ["bash", "-c"]
        args:
            - |
              ray start --head --disable-usage-stats --include-dashboard false 
              # wait for other node to join
              until [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; do
                echo "Waiting..."
                sleep 1
              done
              ray status

              python3 -m vllm.entrypoints.openai.api_server --model ${MODEL_NAME}  --tensor-parallel-size $TENSOR_PARALLEL_SIZE --pipeline-parallel-size $PIPELINE_PARALLEL_SIZE --max-log-len 100  --disable-custom-all-reduce --distributed-executor-backend ray  --disable-frontend-multiprocessing --port ${PORT}  --uvicorn-log-level debug
        env:
          - name: RAY_PORT
            value: "6379"
          - name: HOME
            value: "/tmp"
          - name: RAY_ADDRESS
            value: 127.0.0.1:6379
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          # GPU COUNT per NODE      
          - name: TENSOR_PARALLEL_SIZE
            value: "1"
          # NODE COUNT including head node
          - name: PIPELINE_PARALLEL_SIZE
            value: "2"
          - name: DISTRIBUTED_EXECUTOR_BACKEND
            value: "ray"
          - name: DISABLE_CUSTOM_ALL_REDUCE
            value: "true"
          - name: MAX_SEQUENCE_LENGTH
            value: "8192"
          - name: MAX_NEW_TOKENS
            value: "2048"
          - name: MAX_BATCH_SIZE
            value: "256"
          - name: MAX_CONCURRENT_REQUESTS
            value: "320"
          - name: PORT
            value: "3000"
          - name: MAX_LOG_LEN
            value: "100"
          - name: HF_HUB_CACHE
            value: /tmp
      resources:
        limits:
          cpu: "16"
          memory: 48Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "8"
  workerSpec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: nvidia.com/gpu.product
                  operator: In
                  values:
                    - NVIDIA-A10G
    containers:
      - name: kserve-container
        image: kserve/vllm:latest
        command: ["bash", "-c"]
        args:
          - |
            echo "Attempting to connect to Ray cluster at $RAY_HEAD_ADDRESS ..."
            RAY_HEAD_ADDRESS="${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379"
            ray start --address="$RAY_HEAD_ADDRESS" --block
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        resources:
          limits:
            cpu: "16"
            memory: 48Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "8"
