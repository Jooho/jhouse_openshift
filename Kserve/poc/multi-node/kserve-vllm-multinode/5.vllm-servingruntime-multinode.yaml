apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: kserve-vllm
spec:
  annotations:
    prometheus.kserve.io/port: "3000"
    prometheus.kserve.io/path: "/metrics"
  supportedModelFormats:
    - name: vllm
      version: "1"
      autoSelect: true
      priority: 1
  protocolVersions:
    - v2
    - v1
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  - NVIDIA-A10G    
  containers:
    - name: kserve-container
      # image: vllm/vllm-openai:latest
      image: vllm/vllm-openai:latest
      command: ["bash", "-c"]
      args:
        - |
          ray start --head --disable-usage-stats --include-dashboard false 
          # wait for other node to join
          until [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; do
            echo "Waiting..."
            sleep 1
          done
          ray status

          python3 -m vllm.entrypoints.openai.api_server --model ${MODEL_DIR} --served-model-name ${MODEL_NAME} --tensor-parallel-size ${TENSOR_PARALLEL_SIZE} --pipeline-parallel-size ${PIPELINE_PARALLEL_SIZE} --port ${PORT} 
      env:
        - name: RAY_PORT
          value: "6379"
        - name: HOME
          value: "/tmp"
        - name: RAY_ADDRESS
          value: 127.0.0.1:6379
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: TENSOR_PARALLEL_SIZE
          value: "1"
        - name: PORT
          value: "3000"
        - name: HF_HUB_CACHE
          value: /tmp
      resources:
        limits:
          cpu: "16"
          memory: 48Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "8"
      volumeMounts:
        - name: shm
          mountPath: /dev/shm     
      livenessProbe:
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
        exec:
          command:
            - bash
            - -c
            - |
              # curl --silent --max-time 5 --fail-with-body http://localhost:3000/health &   #vllm 0.6.0 does not have curl 7.75 that has --fail-with-body
              curl --silent --max-time 5 http://localhost:3000/health
      readinessProbe:
        failureThreshold: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
        exec:
          command:
            - bash
            - -c
            - |
              [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]] &
              p1=$!
              # curl --silent --max-time 5 --fail-with-body http://localhost:3000/health &   #vllm 0.6.0 does not have curl 7.75 that has --fail-with-body
              curl --silent --max-time 5 http://localhost:3000/health &
              p2=$!
              wait $p1 $p2
              wait $p1 && wait $p2
      startupProbe:
        failureThreshold: 40
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
        exec:
          command:
            - bash
            - -c
            - |
              # curl --silent --max-time 5 --fail-with-body http://localhost:3000/health &   #vllm 0.6.0 does not have curl 7.75 that has --fail-with-body
              curl --silent --max-time 5 http://localhost:3000/health                  
  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 12Gi
  workerSpec:
    size: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: nvidia.com/gpu.product
                  operator: In
                  values:
                    - NVIDIA-A10G
    containers:
      - name: worker-container
        image: vllm/vllm-openai:latest
        command: ["bash", "-c"]
        args:
          - |
            SECONDS=0

            while true; do              
              if (( SECONDS <= 120 )); then
                if ray health-check --address "${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379" > /dev/null 2>&1; then
                  echo "GCS is ready."
                  break
                fi
                echo "$SECONDS seconds elapsed: Waiting for GCS to be ready."
              else
                if ray health-check --address "${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379"; then
                  echo "GCS is ready. Any error messages above can be safely ignored."
                  break
                fi
                echo "$SECONDS seconds elapsed: Still waiting for GCS to be ready."
                echo "For troubleshooting, refer to the FAQ at https://github.com/ray-project/kuberay/blob/master/docs/guidance/FAQ.md."
              fi
              
              sleep 5
            done

            echo "Attempting to connect to Ray cluster at $RAY_HEAD_ADDRESS ..."
            RAY_HEAD_ADDRESS="${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379"
            ray start --address="$RAY_HEAD_ADDRESS" --block
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace              
        resources:
          limits:
            cpu: "16"
            memory: 48Gi
          requests:
            cpu: "8"
        volumeMounts:
          - name: shm
            mountPath: /dev/shm
        livenessProbe:
          failureThreshold: 3
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 5
          exec:
            command:
              - bash
              - -c
              - |
                [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]
        startupProbe:
          failureThreshold: 12
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 5
          exec:
            command:
              - ray
              - status
    volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 12Gi
