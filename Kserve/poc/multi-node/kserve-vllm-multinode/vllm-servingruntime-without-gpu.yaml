apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: kserve-vllm
spec:
  annotations:
    prometheus.kserve.io/port: "3000"
    prometheus.kserve.io/path: "/metrics"
  supportedModelFormats:
    - name: huggingface
      version: "1"
      autoSelect: true
      priority: 1
  protocolVersions:
    - v2
    - v1
  containers:
    - name: kserve-container
      # image: vllm/vllm-openai:latest
      image: rayproject/ray
      command: ["bash", "-c"]
      args:
        - |
          ray start --head --disable-usage-stats --include-dashboard false 
          # wait for other node to join
          until [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; do
            echo "Waiting..."
            sleep 1
          done
          ray status

          python3 -m vllm.entrypoints.openai.api_server --model ${MODEL_NAME}  --tensor-parallel-size $TENSOR_PARALLEL_SIZE --pipeline-parallel-size $PIPELINE_PARALLEL_SIZE --max-log-len 100  --disable-custom-all-reduce --distributed-executor-backend ray  --disable-frontend-multiprocessing --port ${PORT}  --uvicorn-log-level debug
      env:
        - name: RAY_PORT
          value: "6379"
        - name: HOME
          value: "/tmp"
        - name: RAY_ADDRESS
          value: 127.0.0.1:6379
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # GPU COUNT per NODE      
        - name: TENSOR_PARALLEL_SIZE
          value: "1"
        # NODE COUNT including head node
        - name: PIPELINE_PARALLEL_SIZE
          value: "2"
        - name: DISTRIBUTED_EXECUTOR_BACKEND
          value: "ray"
        - name: DISABLE_CUSTOM_ALL_REDUCE
          value: "true"
        - name: MAX_SEQUENCE_LENGTH
          value: "8192"
        - name: MAX_NEW_TOKENS
          value: "2048"
        - name: MAX_BATCH_SIZE
          value: "256"
        - name: MAX_CONCURRENT_REQUESTS
          value: "320"
        - name: PORT
          value: "3000"
        - name: MAX_LOG_LEN
          value: "100"
        - name: HF_HUB_CACHE
          value: /tmp
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: "1"
      livenessProbe:
        failureThreshold: 3
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
        exec:
          command:
            - bash
            - -c
            - |
              # curl --silent --max-time 5 --fail-with-body http://localhost:3000/health &   #vllm 0.6.0 does not have curl 7.75 that has --fail-with-body
              curl --silent --max-time 5 http://localhost:3000/health
      readinessProbe:
        failureThreshold: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
        exec:
          command:
            - bash
            - -c
            - |
              [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]] &
              p1=$!
              # curl --silent --max-time 5 --fail-with-body http://localhost:3000/health &   #vllm 0.6.0 does not have curl 7.75 that has --fail-with-body
              curl --silent --max-time 5 http://localhost:3000/health &
              p2=$!
              wait $p1 $p2
              wait $p1 && wait $p2
      startupProbe:
        failureThreshold: 40
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
        exec:
          command:
            - bash
            - -c
            - |
              # curl --silent --max-time 5 --fail-with-body http://localhost:3000/health &   #vllm 0.6.0 does not have curl 7.75 that has --fail-with-body
              curl --silent --max-time 5 http://localhost:3000/health          
  workerSpec:
    containers:
      - name: worker-container
        image: rayproject/ray
        command: ["bash", "-c"]
        args:
          - |
            SECONDS=0

            while true; do              
              if (( SECONDS <= 120 )); then
                if ray health-check --address "${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379" > /dev/null 2>&1; then
                  echo "GCS is ready."
                  break
                fi
                echo "$SECONDS seconds elapsed: Waiting for GCS to be ready."
              else
                if ray health-check --address "${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379"; then
                  echo "GCS is ready. Any error messages above can be safely ignored."
                  break
                fi
                echo "$SECONDS seconds elapsed: Still waiting for GCS to be ready."
                echo "For troubleshooting, refer to the FAQ at https://github.com/ray-project/kuberay/blob/master/docs/guidance/FAQ.md."
              fi
              
              sleep 5
            done

            echo "Attempting to connect to Ray cluster at $RAY_HEAD_ADDRESS ..."
            RAY_HEAD_ADDRESS="${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379"
            ray start --address="$RAY_HEAD_ADDRESS" --block
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        resources:
          limits:
            cpu: "2"
            memory: 2Gi
          requests:
            cpu: "1"
        livenessProbe:
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - bash
                - -c
                - |
                  [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]
          startupProbe:
            failureThreshold: 12
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - ray
                - status
