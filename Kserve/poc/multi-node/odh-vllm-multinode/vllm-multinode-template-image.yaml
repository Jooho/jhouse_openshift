apiVersion: template.openshift.io/v1
kind: Template
metadata:
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/ootb: 'true'
  annotations:
    description: vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs 
    openshift.io/display-name: vLLM ServingRuntime for KServe
    openshift.io/provider-display-name: Red Hat, Inc.
    tags: rhods,rhoai,kserve,servingruntime
    template.openshift.io/documentation-url: https://github.com/opendatahub-io/vllm
    template.openshift.io/long-description: This template defines resources needed to deploy vLLM ServingRuntime with KServe in Red Hat OpenShift AI
    opendatahub.io/modelServingSupport: '["single"]'
    opendatahub.io/apiProtocol: 'REST'
  name: vllm-multinode-runtime-template
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: vllm-multinode-runtime
      annotations:
        openshift.io/display-name: vLLM ServingRuntime for KServe
        opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      annotations:
        prometheus.io/port: '8080'
        prometheus.io/path: '/metrics'
      multiModel: false
      supportedModelFormats:
        - autoSelect: true
          name: vLLM
          priority: 2
      containers:
        - name: kserve-container
          image: quay.io/opendatahub/vllm@sha256:7f19dde68eb47abeea155f0d68d4e708f4d93cc91fc632b7a5a0de181d8d193b
          command: ["bash", "-c"]
          args:
            - |
              ray start --head --disable-usage-stats --include-dashboard false 
              # wait for other node to join
              until [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; do
                echo "Waiting..."
                sleep 1
              done
              ray status

              export SERVED_MODEL_NAME=${MODEL_NAME}
              export MODEL_NAME=${MODEL_DIR}

              exec python3 -m vllm_tgis_adapter
          env:
            - name: RAY_PORT
              value: "6379"
            - name: HOME
              value: "/tmp"
            - name: RAY_ADDRESS
              value: 127.0.0.1:6379
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            # GPU COUNT per NODE      
            - name: TENSOR_PARALLEL_SIZE
              value: "1"
            - name: DISTRIBUTED_EXECUTOR_BACKEND
              value: "ray"
            - name: DISABLE_CUSTOM_ALL_REDUCE
              value: "true"
            - name: MAX_SEQUENCE_LENGTH
              value: "8192"
            - name: MAX_NEW_TOKENS
              value: "2048"
            - name: MAX_BATCH_SIZE
              value: "256"
            - name: MAX_CONCURRENT_REQUESTS
              value: "320"
            - name: PORT
              value: "8080"
            - name: MAX_LOG_LEN
              value: "100"
            - name: HF_HUB_CACHE
              value: /tmp
          resources:
            limits:
              cpu: "16"
              memory: 48Gi
            requests:
              cpu: "8"
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 8
            exec:
              command:
                - bash
                - -c
                - |
                  # Check if the registered ray nodes count is the same as PIPELINE_PARALLEL_SIZE
                  if [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; then
                    exit 0
                  else
                    exit 1
                  fi &
                  p1=$!

                  # Check GPU usage (when worker node restarted, it becomes NOT ready)
                  gpu_status=$(ray status | grep GPU)
                  used_gpu=$(echo "$gpu_status" | awk '{print $1}' | cut -d'/' -f1)
                  reserved_gpu=$(echo "$gpu_status" | awk '{print $1}' | cut -d'/' -f2)

                  # Determine health status based on GPU usage
                  if [[ "$used_gpu" != "$reserved_gpu" ]]; then
                      echo "Liveness Probe: Unhealthy - Used: $used_gpu, Reserved: $reserved_gpu"
                      exit 1
                  else
                      exit 0                  
                  fi & # Run this check in the background
                  p2=$!
                  
                  # Wait for all processes to complete
                  wait $p1
                  status1=$?
                  wait $p2
                  status2=$?
                  
                  # Check all return codes
                  if [[ $status1 -ne 0 || $status2 -ne 0 ]]; then
                      echo "One or more processes failed."
                      exit 1
                  else
                      echo "All processes succeeded."
                      exit 0
                  fi
          readinessProbe:
            failureThreshold: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - bash
                - -c
                - |
                  # Check if the registered ray nodes count is the same as PIPELINE_PARALLEL_SIZE
                  if [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; then
                    exit 0
                  else
                    exit 1
                  fi &
                  p1=$!

                  curl --silent --max-time 5 --fail-with-body http://localhost:3000/health &
                  p2=$!
                  
                  # Check GPU usage (when worker node restarted, it becomes NOT ready)
                  gpu_status=$(ray status | grep GPU)
                  used_gpu=$(echo "$gpu_status" | awk '{print $1}' | cut -d'/' -f1)
                  reserved_gpu=$(echo "$gpu_status" | awk '{print $1}' | cut -d'/' -f2)

                  # Determine health status based on GPU usage
                  if [[ "$used_gpu" != "$reserved_gpu" ]]; then
                      echo "Readiness Probe: Unhealthy - Used: $used_gpu, Reserved: $reserved_gpu"
                      exit 1
                  else
                      exit 0                  
                  fi & # Run this check in the background
                  p3=$!

                  # Wait for all processes to complete
                  wait $p1
                  status1=$?
                  wait $p2
                  status2=$?
                  wait $p3
                  status3=$?

                  # Check all return codes
                  if [[ $status1 -ne 0 || $status2 -ne 0 || $status3 -ne 0 ]]; then
                      echo "One or more processes failed."
                      exit 1
                  else
                      echo "All processes succeeded."
                      exit 0
                  fi
          startupProbe:
            failureThreshold: 40
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - bash
                - -c
                - |
                  ray status
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
      workerSpec:
        pipelineParallelSize: 1
        containers:
          - name: worker-container
            image: quay.io/opendatahub/vllm@sha256:7f19dde68eb47abeea155f0d68d4e708f4d93cc91fc632b7a5a0de181d8d193b
            command: ["bash", "-c"]
            args:
              - |
                SECONDS=0

                while true; do              
                  if (( SECONDS <= 120 )); then
                    if ray health-check --address "${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379" > /dev/null 2>&1; then
                      echo "Global Control Service(GCS) is ready."
                      break
                    fi
                    echo "$SECONDS seconds elapsed: Waiting for Global Control Service(GCS) to be ready."
                  else
                    if ray health-check --address "${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379"; then
                      echo "Global Control Service(GCS) is ready. Any error messages above can be safely ignored."
                      break
                    fi
                    echo "$SECONDS seconds elapsed: Still waiting for Global Control Service(GCS) to be ready."
                    echo "For troubleshooting, refer to the FAQ at https://docs.ray.io/en/master/cluster/kubernetes/troubleshooting/troubleshooting.html#kuberay-troubleshootin-guides"
                  fi
                  
                  sleep 5
                done

                echo "Attempting to connect to Ray cluster at $RAY_HEAD_ADDRESS ..."
                RAY_HEAD_ADDRESS="${ISVC_NAME}-head.${POD_NAMESPACE}.svc.cluster.local:6379"
                ray start --address="$RAY_HEAD_ADDRESS" --block
            env:
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
            resources:
              limits:
                cpu: "16"
                memory: 48Gi
              requests:
                cpu: "8"
            livenessProbe:
              failureThreshold: 3
              periodSeconds: 5
              successThreshold: 1
              timeoutSeconds: 5
              exec:
                command:
                  - bash
                  - -c
                  - |
                    # Check if the registered ray nodes count is the same as PIPELINE_PARALLEL_SIZE
                    if [[ $(ray status | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; then
                      exit 0
                    else
                      exit 1
                    fi
            startupProbe:
              failureThreshold: 12
              periodSeconds: 5
              successThreshold: 1
              timeoutSeconds: 5
              exec:
                command:
                  - ray
                  - status
