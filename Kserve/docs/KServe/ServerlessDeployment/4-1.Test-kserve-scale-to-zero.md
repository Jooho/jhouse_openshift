# Test KServe AutoScaling

## Install KServe

Please refer [this doc](./Install-kserve-rhods-on-rosa.md)

## Mandatory Steps
This must be proceeded in the `#Install KServe` but just in case, you should do it again.
~~~
# You should execute this at the jhouse_openshift/Kserve/docs/KServe/ServerlessDeployment
./scripts/setup.sh
source init.sh

cd ${DEMO_HOME}
cp ${KSERVE_MANIFESTS_HOME}/grpc_predict_v2.proto .

oc get ns ${TEST_NS}|| oc new-project ${TEST_NS}
~~~

## Create default ServingRuntimes
~~~
if [[ ! -d ${DEMO_HOME}/kserve ]];then
  cd ${DEMO_HOME}
  git clone https://github.com/kserve/kserve.git 
fi

cd ${DEMO_HOME}/kserve
kustomize build config/runtimes/ |sed "s/ClusterServingRuntime/ServingRuntime/g"|oc create -n kserve-demo -f -
~~~

----

## ScaleToZero

**Deploy tensorflow model (flowers-sample)**
~~~
export MODEL_NAME=tensorflow-flower-sample
oc create -f ${KSERVE_MANIFESTS_HOME}/tensorflow-flower-sample.yaml

wait_for_pods_ready "serving.kserve.io/inferenceservice=${MODEL_NAME}" "${TEST_NS}"
oc wait --for=condition=ready pod -l serving.kserve.io/inferenceservice=${MODEL_NAME} -n ${TEST_NS} --timeout=300s
~~~

**Verify model is working**
~~~
# https://codebeautify.org/base64-to-image-converter
# ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']

ISVC_URL=$(oc get isvc ${MODEL_NAME} -ojsonpath='{.status.url}')
INPUT_DATA=${KSERVE_MANIFESTS_HOME}/tensorflow-v1-input-rest-daisy.json

curl  -k -X POST -H 'accept: application/json'    -H "Content-Type: application/json"   -d @${INPUT_DATA}  ${ISVC_URL}/v1/models/$MODEL_NAME:predict
~~~

- Sample output
~~~
{
    "predictions": [
        {
            "scores": [0.999114931, 9.2098875e-05, 0.000136786606, 0.00033725868, 0.000300534302, 1.84814126e-05],
            "prediction": 0,
            "key": "   1"
        }
    ]
}
~~~

**Set minReplicas to 0 for ScaleToZero**
~~~
oc patch isvc/${MODEL_NAME}  -p '{"spec":{"predictor": {"minReplicas": 0}}}' -n ${TEST_NS} --type merge
oc get pod -w -n ${TEST_NS}

# around 60s~100s, it starts to scale down
~~~

**Send a request to see it is starting**
~~~
# Watch pods 
# oc get pod -w -n ${TEST_NS}
INPUT_DATA=${DEMO_ISV_MANIFESTS_HOME}/tensorflow-v1-input-rest-rose.json

curl -k -X POST -H 'accept: application/json' -H "Content-Type: application/json" -d @${INPUT_DATA}  ${ISVC_URL}/v1/models/$MODEL_NAME:predict
~~~

**Expected output**
~~~
{
    "predictions": [
        {
            "scores": [2.79400103e-09, 3.62791047e-10, 0.999998093, 2.85873849e-08, 1.85485806e-06, 6.4345973e-10],
            "prediction": 2,
            "key": "   1"
        }
    ]
}
~~~

*Tip*
If you want to control the time to keep pod running before scaling down to zero, you can configure it with Knative configmap(config-autoscaler) in knative-serving namespace.
Refer [this doc](https://knative.dev/docs/serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period)


## Cleanup
~~~
oc delete isvc,revisions,pod --all --force --grace-period=0 -n ${TEST_NS}

# optional
# cd openshift-ai-serving-test
#./commands/kserve-rhods-clean.sh
#./commands/kserve-dependencies-clean.sh
~~~
