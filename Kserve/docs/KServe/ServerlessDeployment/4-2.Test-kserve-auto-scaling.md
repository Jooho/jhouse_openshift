# Test KServe AutoScaling

## Install KServe

Please refer [this doc](./Install-kserve-rhods-on-rosa.md)

## Mandatory Steps
This must be proceeded in the `#Install KServe` but just in case, you should do it again.
~~~
# You should execute this at the jhouse_openshift/Kserve/docs/KServe/ServerlessDeployment
./scripts/setup.sh
source init.sh

cd ${DEMO_HOME}
cp ${KSERVE_MANIFESTS_HOME}/grpc_predict_v2.proto .

oc get ns ${TEST_NS}|| oc new-project ${TEST_NS}
~~~

## Create default ServingRuntimes
~~~
if [[ ! -d ${DEMO_HOME}/kserve ]];then
  cd ${DEMO_HOME}
  git clone https://github.com/kserve/kserve.git 
fi

cd ${DEMO_HOME}/kserve
kustomize build config/runtimes/ |sed "s/ClusterServingRuntime/ServingRuntime/g"|oc create -n kserve-demo -f -
~~~

----

## AutoScaling
Apply the tensorflow example CR with scaling target set to 1. Annotation autoscaling.knative.dev/target is the **soft limit** rather than a strictly enforced limit, if there is sudden burst of the requests, this value can be exceeded.
- InferenceService example
  ~~~
  apiVersion: "serving.kserve.io/v1beta1"
  kind: "InferenceService"
  metadata:
    name: "flowers-sample"
  spec:
    predictor:
      scaleTarget: 1 #<===  
      scaleMetric: concurrency  #<===
      model:
        modelFormat:
          name: tensorflow
        storageUri: "gs://kfserving-examples/models/tensorflow/flowers"
  ~~~

- Deploy a sample model for autoscaling demo 
  ~~~
  oc create -f ${KSERVE_MANIFESTS_HOME}/tensorflow-flower-sample.yaml

  MODEL_NAME=tensorflow-flower-sample

  wait_for_pods_ready "serving.kserve.io/inferenceservice=${MODEL_NAME}" "${TEST_NS}"
  oc wait --for=condition=ready pod -l serving.kserve.io/inferenceservice=${MODEL_NAME} -n ${TEST_NS} --timeout=300s
  ~~~

- Verify the model work with simple query ([input_data](https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/tensorflow/input.json))
  ~~~
  ISVC_URL=$(oc get isvc ${MODEL_NAME} -ojsonpath='{.status.url}')
  INPUT_DATA=${KSERVE_MANIFESTS_HOME}/tensorflow-v1-input-rest-daisy.json

  curl  -k -X POST -H 'accept: application/json'    -H "Content-Type: application/json"   -d @${INPUT_DATA}  ${ISVC_URL}/v1/models/$MODEL_NAME:predict
  ~~~

- Load testing to verify AutoScaling
  ~~~
  # setup.sh download the hey binary but if you don't have it, use the following commands:
  # wget https://hey-release.s3.us-east-2.amazonaws.com/hey_linux_amd64
  # chmod 777 hey_linux_amd64
  # sudo mv hey_linux_amd64 /usr/local/bin/hey
  
  hey -z 1s -c 5 -m POST -D $INPUT_DATA ${ISVC_URL}/v1/models/$MODEL_NAME:predict

  ❯ oc get pod
    NAME                                                        READY   STATUS            RESTARTS   AGE
    flowers-sample-predictor-00001-deployment-554bb59d5-2wx72   3/3     Running           0          13s
    flowers-sample-predictor-00001-deployment-554bb59d5-lbqqr   0/3     PodInitializing   0          15s
    flowers-sample-predictor-00001-deployment-554bb59d5-lgg55   0/3     Init:0/1          0          1s
    flowers-sample-predictor-00001-deployment-554bb59d5-mn4wv   3/3     Running           0          3m9s
    flowers-sample-predictor-00001-deployment-554bb59d5-wvsdw   0/3     Init:0/1          0          1s
  ~~~
  This will scale out to unlimited.

- Set maximum pods
  Only 2 pods will be created maximum with this change.
  ~~~
  oc patch isvc ${MODEL_NAME} -p '{"spec":{"predictor":{"maxReplicas":2}}}' --type=merge

  # Cleanup previous revision.
  oc delete revision ${MODEL_NAME}-predictor-00001
  oc delete pod -l serving.knative.dev/revision=${MODEL_NAME}-predictor-00001 --force --grace-period=0

  oc wait --for=condition=ready pod -l serving.knative.dev/revision=${MODEL_NAME}-predictor-00002 -n ${TEST_NS} --timeout=300s

  # Retest
  hey -z 3s -c 5 -m POST -D $INPUT_DATA ${ISVC_URL}/v1/models/$MODEL_NAME:predict

  ❯ oc get pod
    NAME                                                         READY   STATUS     RESTARTS   AGE
    flowers-sample-predictor-00002-deployment-7687c9cd5c-5m57h   3/3     Running    0          61s
    flowers-sample-predictor-00002-deployment-7687c9cd5c-cqws8   0/3     Init:0/1   0          7s
  ~~~
  The pod scaled out up to `maxReplicas`. 

## Cleanup
~~~
oc delete isvc,revisions,pod --all --force --grace-period=0 -n ${TEST_NS}

# optional
# cd openshift-ai-serving-test
#./commands/kserve-rhods-clean.sh
#./commands/kserve-dependencies-clean.sh
~~~
